<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Fake It Till You Make It</title>
    <link rel="shortcut icon" type="image/jpg" href="img/favicon.ico" />
    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css"> -->
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>

</head>

<body>
    <nav class="navbar is-dark" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                    <img src="img/Microsoft-logo.svg" alt="Mixed Reality & AI Lab – Cambridge" style="height: 1.4rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                        Mixed Reality & AI
                    </a>
                </div>
                <div class="navbar-end">
                    <a class="navbar-item" href="http://iccv2021.thecvf.com/">
                        <img class="is-hidden-touch" src="img/iccv-logo.svg" alt="ICCV 2021" style="height: 1.1rem;">
                        <img class="is-hidden-desktop" src="img/iccv-logo-blue.svg" alt="ICCV 2021" style="height: 1.1rem;">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-2 has-text-centered">
                Fake It Till<br class="is-hidden-tablet"> You Make It
            </h1>
            <p class="subtitle is-4 has-text-centered">
                Face analysis in the wild<br class="is-hidden-tablet"> using synthetic data alone
            </p>
            <p class="subtitle is-5 has-text-centered has-text-grey">
                International Conference on<br class="is-hidden-tablet"> Computer Vision 2021
            </p>

            <p class="subtitle is-6 has-text-centered authors" style="line-height: 1.5;">
                <span>
                    <a href="http://www.errollw.com/" class="has-tooltip-bottom" data-tooltip="* denotes equal contribution">Erroll&nbsp;Wood</a><sup>*</sup>
                </span>
                <span>
                    <a href="mailto:tabaltru@microsoft.com" class="has-tooltip-bottom" data-tooltip="* denotes equal contribution">Tadas&nbsp;Baltrusaitis</a><sup>*</sup>
                </span>
                <span>
                    <a href="https://chewitt.me/">Charlie&nbsp;Hewitt</a>
                </span>
                <span>
                    <a href="https://sebastiandziadzio.com">Sebastian&nbsp;Dziadzio</a>
                </span>
                <span>
                    <a href="mailto:matjoh@microsoft.com">Matthew&nbsp;Johnson</a>
                </span>
                <span>
                    <a href="mailto:Virginia.Estellers@microsoft.com">Virginia&nbsp;Estellers</a>
                </span>
                <span>
                    <a href="mailto:tcashman@microsoft.com">Thomas&nbsp;J.&nbsp;Cashman</a>
                </span>
                <span>
                    <a href="https://jamie.shotton.org/">Jamie&nbsp;Shotton</a>
                </span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <!-- <a class="button is-rounded is-link is-light mr-2" disabled>
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a> -->
            <a href="https://arxiv.org/abs/2109.15102" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <a href="https://www.youtube.com/watch?v=wlOMpQe8luQ" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fab fa-youtube"></i></span>
                <span>Video</span>
            </a>
            <a href="https://github.com/microsoft/FaceSynthetics" class="button is-rounded is-link is-light" disabled>
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Dataset</span>
            </a>
        </div>
    </section>
    <section>
        <div class="container is-max-desktop">
            <figure class="image is-16by9">
                <iframe class="has-ratio" width="640" height="360" src="https://youtube.com/embed/wlOMpQe8luQ" frameborder="0" allowfullscreen></iframe>
            </figure>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                <p>We demonstrate that it is possible to perform face-related computer vision in the wild <strong>using synthetic data alone</strong>.</p>
                <p>
                    The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing,
                    domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets.
                </p>
                <p>
                    We describe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity. We train machine learning systems for face-related tasks
                    such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labelling would be impossible.
                </p>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Dataset
            </h1>
            <div class="content has-text-justified-desktop">
                <img class="mb-5" src="img/dataset_samples.jpg">
                <p>
                    Our dataset of <strong>100,000 synthetic faces</strong> with 2D landmark and per-pixel segmentation labels will be available to download soon, for non-commercial research purposes. Please watch this space.
                </p>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <div class="columns">
                <div class="column">
                    <h1 class="title is-5">
                        Face Parsing
                    </h1>
                    <video class="mb-3 is-16by9" width="100%" autoplay muted loop playsinline>
                        <source src="video/face_parsing.mp4" type="video/mp4">
                    </video>
                    <p>
                        Pixel-perfect segmentation labels let us achieve near-SOTA results using off-the-shelf neural networks.
                    </p>
                </div>
                <div class="column">
                    <h1 class="title is-5">
                        Dense Landmarks
                    </h1>
                    <video class="mb-3 is-16by9" width="100%" autoplay muted loop playsinline>
                        <source src="video/dense_landmarks.mp4" type="video/mp4">
                    </video>
                    <p>
                        With synthetic landmark labels, its easy to predict ten times as many landmarks as usual.
                    </p>
                </div>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Method
            </h1>
            <div class="columns is-mobile is-variable is-0-mobile is-0-tablet is-1-desktop is-1-widescreen is-1-fullhd">
                <div class="column"><img src="img/iccv21_process_render_0.png" style="width: 100%;"></div>
                <div class="column"><img src="img/iccv21_process_render_1.png" style="width: 100%;"></div>
                <div class="column"><img src="img/iccv21_process_render_2.png" style="width: 100%;"></div>
                <div class="column"><img src="img/iccv21_process_render_3.png" style="width: 100%;"></div>
                <div class="column"><img src="img/iccv21_process_render_4.png" style="width: 100%;"></div>
                <div class="column"><img src="img/iccv21_process_render_5.png" style="width: 100%;"></div>
                <div class="column"><img src="img/iccv21_process_render_6.png" style="width: 100%;"></div>
            </div>
            <p class="has-text-justified-desktop">
                Our synthetic faces are realistic, diverse, and expressive. Starting with our template face, we randomize the identity, choose a random expression, apply a random texture, and attach random hair and clothing. We finally render the face in a random environment
                using <a href=https://www.cycles-renderer.org>Cycles</a>: a physically-based path tracing renderer.
            </p>
            <div class="columns mt-4">
                <div class="column">
                    <img class="mb-3" src="img/iccv21_identity_samples.png" />
                    <p>
                        Identities sampled from our generative model. We trained this model on a diverse set of high quality scan data.
                    </p>
                </div>
                <div class="column">
                    <img class="mb-3" src="img/iccv21_expression_samples.png" />
                    <p>
                        We randomly pick facial expressions from our performance capture database of 70,000 frames.
                    </p>
                </div>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Diversity
            </h1>
            <div class="content has-text-justified-desktop">
                <img class="mb-3" src="img/muct_9x2.jpg" />
                <p>
                    Synthetically-trained machine learning models should work well across the human population.
                </p>
                <p>
                    By evaluating our landmark detectors on the <a href=http://www.milbo.org/muct/index.html>MUCT Face Database</a>, which exhibits lighting, age, and ethnicity diversity, we see that models trained with synthetic data alone can generalize
                    well to real data of diverse individuals.
                </p>
            </div>
            <div class="columns mt-4">
                <div class="column">
                    <img class="mb-3" src="img/face-model-data-stats.png" />
                </div>
                <div class="column">
                    <div class="content has-text-justified-desktop">
                        <p>
                            In order to generate diverse synthetic data, our generative models must be trained with diverse source data.
                        </p>
                        <p>
                            Here are histograms of self-reported age, gender, and ethnicity in our collection of 3D face scans, which was used to build our face model and texture library. Our collection covers a range of age and ethnicity.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
@misc{wood2021fake,
    title={Fake It Till You Make It: Face analysis in the wild using synthetic data alone},
    author={Erroll Wood and Tadas Baltru\v{s}aitis and Charlie Hewitt and Sebastian Dziadzio and Matthew Johnson and Virginia Estellers and Thomas J. Cashman and Jamie Shotton},
    year={2021},
    eprint={2109.15102},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</pre>
        </div>
    </section>
    <footer class="footer">
        <div class="content has-text-centered">
            <p>
                * denotes equal contribution.&ensp;Work conducted at the <a href=https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge>Mixed Reality & AI Lab &ndash; Cambridge</a>.<br/> The author list in the IEEE digital library
                for ICCV is missing V.E. and M.J. due to a mistake on our side during the publishing process.<br/>
                <img src="img/Microsoft-logo-only.svg" class="mt-5" alt="Mixed Reality & AI Lab – Cambridge" style="height: 2rem;">
            </p>
        </div>
    </footer>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>
